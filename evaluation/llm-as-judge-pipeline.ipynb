{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!ls -la\n",
    "!pip install -qqq cohere"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lw3XaTVz3Dfs",
    "outputId": "54c7af56-8c69-413e-9b65-8e28597ad7b9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "## IMPORTS\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "folder_path = \"data/code_summaries\"\n",
    "output_dir = \"results\"\n",
    "judge_model_name = \"cohere\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "error_log = []\n",
    "\n",
    "co = cohere.ClientV2(\"Z8VuQPeTvunJmHqe5lN65HaEES0BycC9nkCZ6OPJ\")\n"
   ],
   "metadata": {
    "id": "tyNJsahF5ouQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Query function\n",
    "def query_cohere(prompt):\n",
    "    response = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.message.content[0].text.strip()\n",
    "\n",
    "\n",
    "def extract_json_block(text):\n",
    "    brace_stack = []\n",
    "    json_start = None\n",
    "    for i, c in enumerate(text):\n",
    "        if c == '{':\n",
    "            if not brace_stack:\n",
    "                json_start = i\n",
    "            brace_stack.append(c)\n",
    "        elif c == '}':\n",
    "            if brace_stack:\n",
    "                brace_stack.pop()\n",
    "                if not brace_stack and json_start is not None:\n",
    "                    return text[json_start:i+1]\n",
    "    return None\n",
    "\n",
    "def safe_json_extract(text):\n",
    "    try:\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"^```(?:json)?\", \"\", text).strip()\n",
    "        text = re.sub(r\"```$\", \"\", text).strip()\n",
    "\n",
    "        json_str = extract_json_block(text)\n",
    "        if json_str:\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"[Extraction Failed] No JSON block found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[JSONDecodeError] {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Unexpected Error] {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Clean markdown-style code fences\n",
    "def clean_response_text(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove markdown code fences\n",
    "    if text.startswith(\"```json\") or text.startswith(\"```\"):\n",
    "        text = re.sub(r\"^```(?:json)?\", \"\", text).strip()\n",
    "        text = re.sub(r\"```$\", \"\", text).strip()\n",
    "\n",
    "    # Extract first JSON object using regex\n",
    "    match = re.search(r\"{.*?}\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return text"
   ],
   "metadata": {
    "id": "E_8jW4e95taz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt: compare English vs non-English\n",
    "def build_comparison_prompt(code, eng_summary, target_summary, target_language):\n",
    "    return f\"\"\"\n",
    "You are a multilingual software expert evaluating two summaries of the same code snippet:\n",
    "\n",
    "    One is in English (reference).\n",
    "    One is in {target_language}.\n",
    "\n",
    "--- CODE ---\n",
    "{code}\n",
    "\n",
    "--- ENGLISH SUMMARY ---\n",
    "{eng_summary}\n",
    "\n",
    "--- {target_language.upper()} SUMMARY ---\n",
    "{target_summary}\n",
    "\n",
    "Evaluate both summaries on:\n",
    "\n",
    "    Accuracy\n",
    "    Completeness\n",
    "    Terminology Fidelity\n",
    "    Language Quality\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "    Give each a score from 1 to 5 per criterion\n",
    "    Calculate overall scores (1–5)\n",
    "\n",
    "⚠️ RESPONSE RULES (STRICT):\n",
    "\n",
    "    Respond ONLY with a valid JSON object\n",
    "    DO NOT add explanations, markdown, or comments\n",
    "    Response MUST start and end with curly braces\n",
    "\n",
    "EXAMPLE FORMAT:\n",
    "{{\n",
    "  \"score_english\": {{\n",
    "    \"accuracy\": 5,\n",
    "    \"completeness\": 5,\n",
    "    \"terminology\": 4,\n",
    "    \"language_quality\": 5,\n",
    "    \"overall_score\": 5\n",
    "  }},\n",
    "  \"score_non_english\": {{\n",
    "    \"accuracy\": 4,\n",
    "    \"completeness\": 4,\n",
    "    \"terminology\": 3,\n",
    "    \"language_quality\": 4,\n",
    "    \"overall_score\": 4\n",
    "  }}\n",
    "}}\n",
    "\n",
    "REPEAT: Only respond with the JSON shown above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Prompt: evaluate only non-English summary\n",
    "def build_single_summary_prompt(code, target_summary, target_language):\n",
    "    return f\"\"\"\n",
    "You are a multilingual software expert evaluating a summary written in {target_language} for the following code.\n",
    "\n",
    "--- CODE ---\n",
    "{code}\n",
    "\n",
    "--- SUMMARY ({target_language}) ---\n",
    "{target_summary}\n",
    "\n",
    "Evaluate the summary on:\n",
    "\n",
    "    Accuracy\n",
    "    Completeness\n",
    "    Terminology Fidelity\n",
    "    Language Quality\n",
    "\n",
    "⚠️ RESPONSE RULES (STRICT):\n",
    "\n",
    "    Respond ONLY with a JSON object (no explanation)\n",
    "    Start and end with curly braces\n",
    "\n",
    "EXAMPLE FORMAT:\n",
    "{{\n",
    "  \"accuracy\": 4,\n",
    "  \"completeness\": 5,\n",
    "  \"terminology\": 4,\n",
    "  \"language_quality\": 5,\n",
    "  \"overall_score\": 4\n",
    "}}\n",
    "\n",
    "REPEAT: No text or markdown. Only the JSON.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "rNKEzgRN2-Hw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ─── Evaluator ───────────────────────────────────────────────────\n",
    "def evaluate_entry(task):\n",
    "    entry, model_folder, filename, code_lang, idx, key, target_language, model_fn = task\n",
    "    results = {\"comparison\": None, \"single\": None}\n",
    "    code = entry.get(\"code\", \"\")\n",
    "    english_summary = entry.get(\"summary_english\", \"\")\n",
    "    target_summary = entry[key]\n",
    "    sample_id = entry.get(\"id\", f\"{code_lang}_{idx}\")\n",
    "    model_name = entry.get(\"model_name\", model_folder)\n",
    "\n",
    "    try:\n",
    "        prompt_comp = build_comparison_prompt(code, english_summary, target_summary, target_language)\n",
    "        response_comp = model_fn(prompt_comp)\n",
    "        parsed_comp = safe_json_extract(response_comp)\n",
    "        if not parsed_comp:\n",
    "          raise RuntimeError(f\"[Parsing Failed] Malformed JSON for sample {sample_id}\")\n",
    "\n",
    "        results[\"comparison\"] = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"model_folder_name\": model_folder,\n",
    "            \"model_name\": model_name,\n",
    "            \"programming_language\": code_lang,\n",
    "            \"language\": target_language,\n",
    "            \"reference_summary\": english_summary,\n",
    "            \"generated_summary\": target_summary,\n",
    "            \"judge_model\": judge_model_name,\n",
    "            \"llm_eng_accuracy\": parsed_comp[\"score_english\"][\"accuracy\"],\n",
    "            \"llm_eng_completeness\": parsed_comp[\"score_english\"][\"completeness\"],\n",
    "            \"llm_eng_terminology\": parsed_comp[\"score_english\"][\"terminology\"],\n",
    "            \"llm_eng_language_quality\": parsed_comp[\"score_english\"][\"language_quality\"],\n",
    "            \"llm_eng_overall_score\": parsed_comp[\"score_english\"][\"overall_score\"],\n",
    "            \"llm_mt_accuracy\": parsed_comp[\"score_non_english\"][\"accuracy\"],\n",
    "            \"llm_mt_completeness\": parsed_comp[\"score_non_english\"][\"completeness\"],\n",
    "            \"llm_mt_terminology\": parsed_comp[\"score_non_english\"][\"terminology\"],\n",
    "            \"llm_mt_language_quality\": parsed_comp[\"score_non_english\"][\"language_quality\"],\n",
    "            \"llm_mt_overall_score\": parsed_comp[\"score_non_english\"][\"overall_score\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        error_log.append(f\"[Comparison ERROR] sample_id={sample_id} lang={target_language} → {str(e)}\\nRAW_RESPONSE:\\n{repr(response_comp[:300])}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        prompt_single = build_single_summary_prompt(code, target_summary, target_language)\n",
    "        response_single = model_fn(prompt_single)\n",
    "        parsed_single = safe_json_extract(response_single)\n",
    "        if not parsed_single:\n",
    "          raise RuntimeError(f\"[Parsing Failed] Malformed JSON for sample {sample_id}\")\n",
    "\n",
    "        results[\"single\"] = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"model_folder_name\": model_folder,\n",
    "            \"model_name\": model_name,\n",
    "            \"programming_language\": code_lang,\n",
    "            \"language\": target_language,\n",
    "            \"reference_summary\": english_summary,\n",
    "            \"generated_summary\": target_summary,\n",
    "            \"judge_model\": judge_model_name,\n",
    "            \"llm_single_accuracy\": parsed_single[\"accuracy\"],\n",
    "            \"llm_single_completeness\": parsed_single[\"completeness\"],\n",
    "            \"llm_single_terminology\": parsed_single[\"terminology\"],\n",
    "            \"llm_single_language_quality\": parsed_single[\"language_quality\"],\n",
    "            \"llm_single_overall_score\": parsed_single[\"overall_score\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        error_log.append(f\"[Single ERROR] sample_id={sample_id} lang={target_language} → {str(e)}\\nRAW_RESPONSE:\\n{repr(response_single[:300])}\")\n",
    "\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# ─── Task Collection ─────────────────────────────────────────────\n",
    "tasks = []\n",
    "for model_folder in os.listdir(folder_path):\n",
    "    model_path = os.path.join(folder_path, model_folder)\n",
    "    if not os.path.isdir(model_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(model_path):\n",
    "        if not filename.endswith(\".json\") or \"all_languages_combined\" in filename:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(model_path, filename)\n",
    "        code_lang = filename.split('_')[0]\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for idx, entry in enumerate(data):\n",
    "            if not entry.get(\"code\") or not entry.get(\"summary_english\"):\n",
    "                sample_id = entry.get(\"id\", f\"{code_lang}_{idx}\")\n",
    "                error_log.append(f\"[Skipped] Missing code or English summary → {sample_id}\")\n",
    "                continue\n",
    "\n",
    "            for key in entry:\n",
    "                if key.startswith(\"summary_\") and key != \"summary_english\":\n",
    "                    target_language = key.replace(\"summary_\", \"\")\n",
    "                    tasks.append((entry, model_folder, filename, code_lang, idx, key, target_language, query_cohere))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-NCJfk4XmO1",
    "outputId": "22f59a56-b519-45ef-9c2c-e20e9eb8ef2a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ─── Evaluation Execution ────────────────────────────────────────\n",
    "\n",
    "comparison_results = {judge_model_name: []}\n",
    "single_results = {judge_model_name: []}\n",
    "\n",
    "print(f\"\\n🚀 Starting evaluation of {len(tasks)} entries using 30 threads...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    futures = [executor.submit(evaluate_entry, task) for task in tasks]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(tasks), desc=\"Evaluating\", ncols=100):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result[\"comparison\"]:\n",
    "                comparison_results[judge_model_name].append(result[\"comparison\"])\n",
    "            if result[\"single\"]:\n",
    "                single_results[judge_model_name].append(result[\"single\"])\n",
    "        except Exception as e:\n",
    "            error_log.append(f\"[Unhandled Future Exception] → {str(e)}\")\n",
    "\n",
    "\n",
    "# ─── Save CSV Results ────────────────────────────────────────────\n",
    "def save_csv(path, rows):\n",
    "    if rows:\n",
    "        with open(path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"Saved: {path}\")\n",
    "\n",
    "save_csv(os.path.join(output_dir, f\"comparison_{judge_model_name}.csv\"), comparison_results[judge_model_name])\n",
    "save_csv(os.path.join(output_dir, f\"single_{judge_model_name}.csv\"), single_results[judge_model_name])\n",
    "\n",
    "# ─── Save Error Log ──────────────────────────────────────────────\n",
    "if error_log:\n",
    "    error_log_path = os.path.join(output_dir, f\"llm_judge_errors_{judge_model_name}.log\")\n",
    "    with open(error_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in error_log:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"Errors encountered. Logged to: {error_log_path}\")\n",
    "else:\n",
    "    print(\"No errors logged.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"results/comparison_cohere.csv\") as f:\n",
    "\n",
    "    print(f.read())\n"
   ],
   "metadata": {
    "id": "QaI1PFWcW-90",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "36b2d9e4-ee91-4760-d3c7-ff7b482a8729"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "mOJpf3YFXmUI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Z1mCTZ2LXmW2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FASJD8WlXo5Y"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TOoQ4WqOXo7w"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "DkFsNT0VXo-G"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "LKvFZ_tKXpAu"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
