# Multilingual Code Summarization Evaluation

This repository contains all raw data and visualizations related to the evaluation of multilingual code summaries generated by large language models (LLMs).

## ðŸ“‚ File Descriptions

| File Name                   | Description                                                                 |
|----------------------------|-----------------------------------------------------------------------------|
| `all_scores.json`          | Full evaluation results across all models, programming languages, and natural languages. |
| `model_ranking.png`        | Bar chart showing overall (raw average) scores per model.                   |
| `metric_wise_perf.png`     | Heatmap showing performance of each model across individual metrics.        |
| `README.md`                | This file â€” explains what each file contains.                               |

## ðŸ§ª Purpose

The goal of this project is to evaluate how well LLMs perform at code summarization across multiple programming and human languages, and to identify which metrics are most effective in multilingual evaluation contexts.
